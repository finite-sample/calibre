{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plateau Diagnostics Demo\n",
    "\n",
    "This notebook demonstrates the use of isotonic regression plateau diagnostics to distinguish between noise-based flattening (good) and limited-data flattening (bad).\n",
    "\n",
    "## Overview\n",
    "\n",
    "When isotonic regression creates flat regions (plateaus) in calibration curves, it could be for two reasons:\n",
    "\n",
    "1. **Noise-based flattening (good)**: Adjacent scores truly have similar risks, and pooling reduces variance without losing meaningful resolution.\n",
    "2. **Limited-data flattening (bad)**: Adjacent scores have different risks, but the calibration sample is too small to detect the difference.\n",
    "\n",
    "This package provides comprehensive diagnostics to help distinguish between these cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "# Import calibre diagnostics\n",
    "from calibre import (\n",
    "    IsotonicRegressionWithDiagnostics,\n",
    "    IsotonicDiagnostics,\n",
    "    analyze_plateaus,\n",
    "    NearlyIsotonicRegression,\n",
    "    tie_preservation_score,\n",
    "    plateau_quality_score,\n",
    "    calibration_diversity_index,\n",
    "    progressive_sampling_diversity\n",
    ")\n",
    "\n",
    "# Import visualization (optional)\n",
    "try:\n",
    "    from calibre.visualization import (\n",
    "        plot_plateau_diagnostics,\n",
    "        plot_progressive_sampling,\n",
    "        plot_calibration_comparison\n",
    "    )\n",
    "    HAS_VIZ = True\n",
    "except ImportError:\n",
    "    print(\"Visualization module requires matplotlib. Install with: pip install matplotlib\")\n",
    "    HAS_VIZ = False\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Calibre plateau diagnostics demo loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Data\n",
    "\n",
    "Let's create two scenarios:\n",
    "- **Scenario A**: Data with genuine flat regions (noise-based flattening)\n",
    "- **Scenario B**: Data with smooth trends but small sample size (limited-data flattening)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_genuine_plateau_data(n=200, noise_level=0.05):\n",
    "    \"\"\"Create data with genuine flat regions.\"\"\"\n",
    "    X = np.sort(np.random.uniform(0, 1, n))\n",
    "    \n",
    "    # Create true probabilities with intentional flat regions\n",
    "    y_true = np.zeros(n)\n",
    "    y_true[:n//4] = 0.1  # Flat low region\n",
    "    y_true[n//4:n//2] = np.linspace(0.1, 0.4, n//4)  # Rising\n",
    "    y_true[n//2:3*n//4] = 0.4  # Flat middle region\n",
    "    y_true[3*n//4:] = np.linspace(0.4, 0.8, n//4)  # Rising\n",
    "    \n",
    "    # Add small amount of noise\n",
    "    y_true += np.random.normal(0, noise_level, n)\n",
    "    y_true = np.clip(y_true, 0, 1)\n",
    "    \n",
    "    # Generate binary outcomes\n",
    "    y_binary = np.random.binomial(1, y_true)\n",
    "    \n",
    "    return X, y_binary, y_true\n",
    "\n",
    "def create_smooth_small_data(n=50):\n",
    "    \"\"\"Create smooth data with small sample size.\"\"\"\n",
    "    X = np.sort(np.random.uniform(0, 1, n))\n",
    "    \n",
    "    # Smooth sigmoid-like curve\n",
    "    y_true = 1 / (1 + np.exp(-8 * (X - 0.5)))\n",
    "    \n",
    "    # Generate binary outcomes\n",
    "    y_binary = np.random.binomial(1, y_true)\n",
    "    \n",
    "    return X, y_binary, y_true\n",
    "\n",
    "# Generate both scenarios\n",
    "X_genuine, y_genuine, y_true_genuine = create_genuine_plateau_data()\n",
    "X_small, y_small, y_true_small = create_smooth_small_data()\n",
    "\n",
    "print(f\"Genuine plateau data: {len(X_genuine)} samples\")\n",
    "print(f\"Small sample data: {len(X_small)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Isotonic Regression with Diagnostics\n",
    "\n",
    "Let's start with the simple wrapper that automatically runs diagnostics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario A: Genuine plateaus\n",
    "print(\"=== Scenario A: Genuine Plateau Data ===\")\n",
    "cal_genuine = IsotonicRegressionWithDiagnostics(\n",
    "    enable_diagnostics=True,\n",
    "    n_bootstraps=50,  # Reduced for demo speed\n",
    "    random_state=42\n",
    ")\n",
    "cal_genuine.fit(X_genuine, y_genuine)\n",
    "\n",
    "print(\"\\nDiagnostic Summary:\")\n",
    "print(cal_genuine.plateau_summary())\n",
    "\n",
    "# Get calibrated predictions\n",
    "y_cal_genuine = cal_genuine.transform(X_genuine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario B: Small sample data\n",
    "print(\"=== Scenario B: Small Sample Data ===\")\n",
    "cal_small = IsotonicRegressionWithDiagnostics(\n",
    "    enable_diagnostics=True,\n",
    "    n_bootstraps=50,\n",
    "    random_state=42\n",
    ")\n",
    "cal_small.fit(X_small, y_small)\n",
    "\n",
    "print(\"\\nDiagnostic Summary:\")\n",
    "print(cal_small.plateau_summary())\n",
    "\n",
    "y_cal_small = cal_small.transform(X_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Diagnostic Analysis\n",
    "\n",
    "For more detailed analysis, we can use the `IsotonicDiagnostics` class directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for more thorough analysis (train/test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_genuine, y_genuine, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Run comprehensive diagnostics\n",
    "diagnostics = IsotonicDiagnostics(\n",
    "    n_bootstraps=50,\n",
    "    n_splits=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "results = diagnostics.analyze(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"Detailed diagnostic results:\")\n",
    "print(f\"Number of plateaus: {results['n_plateaus']}\")\n",
    "print(f\"Classification counts: {results['classification_counts']}\")\n",
    "\n",
    "if results['n_plateaus'] > 0:\n",
    "    print(\"\\nPlateau details:\")\n",
    "    for plateau in results['plateaus']:\n",
    "        print(f\"  Plateau {plateau['plateau_id'] + 1}:\")\n",
    "        print(f\"    X range: {plateau['x_range']}\")\n",
    "        print(f\"    Classification: {plateau['classification']}\")\n",
    "        if plateau['tie_stability'] is not None:\n",
    "            print(f\"    Tie stability: {plateau['tie_stability']:.3f}\")\n",
    "        if plateau['conditional_auc'] is not None:\n",
    "            print(f\"    Conditional AUC: {plateau['conditional_auc']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Diagnostic Metrics\n",
    "\n",
    "Let's explore the specific diagnostic metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs calibrated predictions\n",
    "iso_basic = IsotonicRegression()\n",
    "iso_basic.fit(X_genuine, y_genuine)\n",
    "y_cal_basic = iso_basic.transform(X_genuine)\n",
    "\n",
    "# Tie preservation score\n",
    "tie_score = tie_preservation_score(X_genuine, y_cal_basic)\n",
    "print(f\"Tie preservation score: {tie_score:.3f}\")\n",
    "\n",
    "# Plateau quality score\n",
    "quality_score = plateau_quality_score(X_genuine, y_genuine, y_cal_basic)\n",
    "print(f\"Plateau quality score: {quality_score:.3f}\")\n",
    "\n",
    "# Calibration diversity\n",
    "diversity_orig = calibration_diversity_index(X_genuine)\n",
    "diversity_cal = calibration_diversity_index(y_cal_basic)\n",
    "diversity_relative = calibration_diversity_index(y_cal_basic, diversity_orig)\n",
    "\n",
    "print(f\"Original diversity: {diversity_orig:.3f}\")\n",
    "print(f\"Calibrated diversity: {diversity_cal:.3f}\")\n",
    "print(f\"Relative diversity: {diversity_relative:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Progressive Sampling Analysis\n",
    "\n",
    "This helps distinguish limited-data flattening by showing how diversity changes with sample size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progressive sampling analysis\n",
    "sample_sizes, diversities = progressive_sampling_diversity(\n",
    "    X_genuine, y_genuine,\n",
    "    sample_sizes=[50, 100, 150, 200],\n",
    "    n_trials=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Progressive sampling results:\")\n",
    "for size, div in zip(sample_sizes, diversities):\n",
    "    print(f\"  Sample size {size}: diversity = {div:.3f}\")\n",
    "\n",
    "# Interpret trend\n",
    "slope = (diversities[-1] - diversities[0]) / (sample_sizes[-1] - sample_sizes[0])\n",
    "if slope > 0.001:\n",
    "    print(\"\\nInterpretation: Increasing diversity suggests potential limited-data flattening\")\n",
    "elif slope < -0.001:\n",
    "    print(\"\\nInterpretation: Decreasing diversity (unusual pattern)\")\n",
    "else:\n",
    "    print(\"\\nInterpretation: Stable diversity suggests genuine flatness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison with Alternative Methods\n",
    "\n",
    "Let's compare strict isotonic regression with softer alternatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different calibration methods\n",
    "from calibre import NearlyIsotonicRegression, RegularizedIsotonicRegression\n",
    "\n",
    "# Fit different calibrators\n",
    "iso_strict = IsotonicRegression()\n",
    "iso_nearly = NearlyIsotonicRegression(lam=1.0)\n",
    "iso_reg = RegularizedIsotonicRegression(alpha=0.1)\n",
    "\n",
    "calibrators = {\n",
    "    'Strict Isotonic': iso_strict.fit(X_genuine, y_genuine),\n",
    "    'Nearly Isotonic': iso_nearly.fit(X_genuine, y_genuine),\n",
    "    'Regularized': iso_reg.fit(X_genuine, y_genuine)\n",
    "}\n",
    "\n",
    "# Compare diversity and calibration error\n",
    "from calibre import mean_calibration_error\n",
    "\n",
    "print(\"Method comparison:\")\n",
    "for name, cal in calibrators.items():\n",
    "    try:\n",
    "        y_pred = cal.transform(X_genuine)\n",
    "        diversity = calibration_diversity_index(y_pred)\n",
    "        error = mean_calibration_error(y_genuine, y_pred)\n",
    "        n_unique = len(np.unique(y_pred))\n",
    "        \n",
    "        print(f\"  {name}:\")\n",
    "        print(f\"    Diversity: {diversity:.3f}\")\n",
    "        print(f\"    Calibration error: {error:.3f}\")\n",
    "        print(f\"    Unique values: {n_unique}/{len(y_pred)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {name}: Error - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization (if matplotlib available)\n",
    "\n",
    "Let's create some visualizations to better understand the diagnostics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_VIZ:\n",
    "    # Plot comprehensive diagnostic results\n",
    "    y_cal_for_plot = iso_strict.transform(X_genuine)\n",
    "    fig1 = plot_plateau_diagnostics(results, X_genuine, y_cal_for_plot)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot progressive sampling results\n",
    "    fig2 = plot_progressive_sampling(sample_sizes, diversities)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot calibration method comparison\n",
    "    fig3 = plot_calibration_comparison(X_genuine, y_genuine, calibrators)\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"Visualization not available. Install matplotlib to see plots.\")\n",
    "    \n",
    "    # Simple text-based visualization\n",
    "    print(\"\\nSimple calibration curve comparison:\")\n",
    "    X_sample = X_genuine[::10]  # Sample every 10th point\n",
    "    \n",
    "    for name, cal in calibrators.items():\n",
    "        try:\n",
    "            y_sample = cal.transform(X_sample)\n",
    "            print(f\"\\n{name}:\")\n",
    "            for i in range(0, len(X_sample), 2):\n",
    "                print(f\"  X={X_sample[i]:.2f} -> Y={y_sample[i]:.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"{name}: Error - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Practical Decision Framework\n",
    "\n",
    "Based on the diagnostic results, here's how to make practical decisions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_calibration_method(diagnostic_results, diversity_trend_slope=None):\n",
    "    \"\"\"Provide calibration method recommendations based on diagnostics.\"\"\"\n",
    "    \n",
    "    if diagnostic_results['n_plateaus'] == 0:\n",
    "        return \"Standard isotonic regression (no plateaus detected)\"\n",
    "    \n",
    "    counts = diagnostic_results['classification_counts']\n",
    "    \n",
    "    # Count concerning plateaus\n",
    "    concerning = counts['limited_data'] + counts['inconclusive']\n",
    "    total = sum(counts.values())\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    if concerning == 0:\n",
    "        recommendations.append(\"âœ… Standard isotonic regression (all plateaus appear genuine)\")\n",
    "    elif concerning / total > 0.5:\n",
    "        recommendations.append(\"âš ï¸  Consider softer calibration methods:\")\n",
    "        recommendations.append(\"   - Nearly isotonic regression (allows small violations)\")\n",
    "        recommendations.append(\"   - Regularized isotonic regression\")\n",
    "        recommendations.append(\"   - I-spline calibration\")\n",
    "    else:\n",
    "        recommendations.append(\"ðŸ¤” Mixed evidence - consider:\")\n",
    "        recommendations.append(\"   - Cross-validation between strict and soft methods\")\n",
    "        recommendations.append(\"   - Collecting more calibration data if possible\")\n",
    "    \n",
    "    # Additional recommendations based on diversity trend\n",
    "    if diversity_trend_slope is not None:\n",
    "        if diversity_trend_slope > 0.001:\n",
    "            recommendations.append(\"ðŸ“ˆ Increasing diversity with sample size suggests limited-data flattening\")\n",
    "            recommendations.append(\"   -> Strongly recommend collecting more data or using softer methods\")\n",
    "        elif diversity_trend_slope < -0.001:\n",
    "            recommendations.append(\"ðŸ“‰ Unusual decreasing diversity pattern - investigate data quality\")\n",
    "    \n",
    "    return \"\\n\".join(recommendations)\n",
    "\n",
    "# Get recommendations for our data\n",
    "slope = (diversities[-1] - diversities[0]) / (sample_sizes[-1] - sample_sizes[0])\n",
    "recommendations = recommend_calibration_method(results, slope)\n",
    "\n",
    "print(\"=== CALIBRATION METHOD RECOMMENDATIONS ===\")\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Best Practices\n",
    "\n",
    "### Key Diagnostic Indicators:\n",
    "\n",
    "1. **Tie Stability (Bootstrap)**: \n",
    "   - High (>0.7): Suggests genuine flatness\n",
    "   - Low (<0.3): Suggests limited-data flattening\n",
    "\n",
    "2. **Conditional AUC Among Tied Pairs**:\n",
    "   - Close to 0.5: Supports noise-based flattening\n",
    "   - Much above 0.5: Suggests limited-data flattening\n",
    "\n",
    "3. **Progressive Sampling Diversity**:\n",
    "   - Stable: Supports genuine flatness\n",
    "   - Increasing: Suggests limited-data flattening\n",
    "\n",
    "4. **Minimum Detectable Difference (MDD)**:\n",
    "   - Compare with domain knowledge of plausible effect sizes\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Always run diagnostics** when using isotonic regression\n",
    "2. **Use multiple diagnostic criteria** together, not individually\n",
    "3. **Consider domain knowledge** about expected effect sizes\n",
    "4. **Cross-validate** between strict and soft calibration methods\n",
    "5. **Collect more data** when limited-data flattening is suspected\n",
    "6. **Document your calibration decisions** based on diagnostic evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ‰ Plateau diagnostics demo completed!\")\n",
    "print(\"\\nKey takeaways:\")\n",
    "print(\"1. Not all plateaus are created equal\")\n",
    "print(\"2. Diagnostics help distinguish genuine vs. artifactual flattening\")\n",
    "print(\"3. Multiple complementary tests provide robust evidence\")\n",
    "print(\"4. Consider both statistical and domain-specific evidence\")\n",
    "print(\"5. When in doubt, prefer softer calibration methods\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}